{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e386eb62-064c-4bd2-ab79-93a1e55c6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义作为拟合目标的函数(sinx + x)\n",
    "class fun:\n",
    "    def __init__(self, noise_level=0.0):\n",
    "        '''noise_level为噪声强度，默认无噪声'''\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = np.sin(x)\n",
    "        if self.noise_level > 0:\n",
    "            noise = np.random.normal(0, self.noise_level, size=x.shape)\n",
    "            y += noise\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ab07f2-362a-4240-9fd8-0a83ca99b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(domain, noise_level=0.0, num_train=1000, num_test=200):\n",
    "    \"\"\"\n",
    "    生成训练数据和测试数据。\n",
    "    \n",
    "    :param domain: 定义域，即 x 的范围，元组类型 (x_min, x_max)\n",
    "    :param noise_level: 噪声强度，默认为 0，即不添加噪声\n",
    "    :param num_train: 训练集的样本数\n",
    "    :param num_test: 测试集的样本数\n",
    "    :return: 返回训练数据和测试数据\n",
    "    \"\"\"\n",
    "    # 创建 函数 类的实例\n",
    "    func = fun(noise_level=noise_level)\n",
    "    \n",
    "    # 生成训练集\n",
    "    x_train = np.random.uniform(domain[0], domain[1], num_train)\n",
    "    y_train = func(x_train)\n",
    "    \n",
    "    # 生成测试集\n",
    "    x_test = np.random.uniform(domain[0], domain[1], num_test)\n",
    "    y_test = func(x_test)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "416e052d-f561-4047-b236-d3adca5a13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        \"\"\"\n",
    "        初始化神经网络的结构和参数。\n",
    "        :param input_size: 输入层的大小\n",
    "        :param hidden_size1: 第一隐藏层的大小\n",
    "        :param hidden_size2: 第二隐藏层的大小\n",
    "        :param output_size: 输出层的大小\n",
    "        \"\"\"\n",
    "        # 初始化权重和偏置\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "        self.W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU 激活函数\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"ReLU 激活函数的导数\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        self.z1 = np.dot(x, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        return self.z3\n",
    "\n",
    "    def backward(self, x, y, learning_rate=0.01):\n",
    "        \"\"\"反向传播，计算梯度并更新权重\"\"\"\n",
    "        m = x.shape[0]\n",
    "\n",
    "        # 计算损失函数关于输出的梯度\n",
    "        dz3 = self.z3 - y\n",
    "        dW3 = np.dot(self.a2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # 计算损失函数关于隐藏层2的梯度\n",
    "        dz2 = np.dot(dz3, self.W3.T) * self.relu_derivative(self.z2)\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # 计算损失函数关于隐藏层1的梯度\n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.relu_derivative(self.z1)\n",
    "        dW1 = np.dot(x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=1000, learning_rate=0.01):\n",
    "        \"\"\"训练神经网络\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(x_train.shape[0]):\n",
    "                x = x_train[i]\n",
    "                y = y_train[i]\n",
    "                y_pred = self.forward(x)\n",
    "    \n",
    "                # 反向传播\n",
    "                self.backward(x, y, learning_rate)\n",
    "\n",
    "            # 每100次打印一次损失\n",
    "            if epoch % 100 == 0:\n",
    "                # 使用 RMSE 作为损失函数\n",
    "                loss = np.sqrt(np.mean((y_pred - y_train) ** 2))  # 均方根误差\n",
    "                print(f\"Epoch {epoch}, Loss (RMSE): {loss}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"用于推理\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e267df6e-4ee1-4413-9fc4-50871d030750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(200,)\n",
      "Epoch 0, Loss (RMSE): 0.6830415783686106\n",
      "Epoch 100, Loss (RMSE): 0.6629739890664503\n",
      "Epoch 200, Loss (RMSE): 0.6629739590407072\n",
      "Epoch 300, Loss (RMSE): 0.6629739377333441\n",
      "Epoch 400, Loss (RMSE): 0.6629739164995102\n",
      "Epoch 500, Loss (RMSE): 0.6629738952660054\n",
      "Epoch 600, Loss (RMSE): 0.6629738739739166\n",
      "Epoch 700, Loss (RMSE): 0.6629738525789425\n",
      "Epoch 800, Loss (RMSE): 0.6629738310390203\n",
      "Epoch 900, Loss (RMSE): 0.662973809330617\n",
      "Training Loss: 0.4395345545677529\n",
      "Test Loss: 0.5028833938998686\n"
     ]
    }
   ],
   "source": [
    "domain = (0, 10)\n",
    "noise_level = 0.0\n",
    "(x_train, y_train), (x_test, y_test) = generate_data(domain, noise_level)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_test = x_test.reshape(-1, 1)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "\n",
    "# 定义神经网络\n",
    "input_size = 1\n",
    "hidden_size1 = 64  # 第一隐藏层大小\n",
    "hidden_size2 = 32  # 第二隐藏层大小\n",
    "output_size = 1\n",
    "model = SimpleNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "model.train(x_train, y_train, epochs=1000, learning_rate=0.0001)\n",
    "\n",
    "# 测试神经网络的表现\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "train_loss = np.mean((y_pred_train - y_train) ** 2)\n",
    "test_loss = np.mean((y_pred_test - y_test) ** 2)\n",
    "\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c187b09-b82a-49af-8be5-3577cf7d88ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
